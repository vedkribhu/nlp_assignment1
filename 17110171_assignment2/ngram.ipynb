{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open('at.txt', 'r').read()\n",
    "sent_tokenize_list = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_, test_ =  train_test_split(sent_tokenize_list, test_size = 0.2, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178675\n"
     ]
    }
   ],
   "source": [
    "#I process the sentence tokenized list. All sentences are first tokenized into words. \n",
    "#<s> and </s> characters are added to mark begin and end of each sentence.\n",
    "#All sentences are then put in a single list called vocabullary.\n",
    "ls = []\n",
    "sent_tokenize_list = [token.lower() for token in train_]\n",
    "vocabulary = []\n",
    "for sentence in sent_tokenize_list:\n",
    "    sentence = \"< \"+sentence+\" >\"\n",
    "    words = word_tokenize(sentence)\n",
    "    for word in words:\n",
    "        if word == ',' or word == '.':\n",
    "            pass\n",
    "        elif word == '<':\n",
    "            vocabulary.append(\"<s>\")\n",
    "        elif word == '>':\n",
    "            vocabulary.append(\"</s>\")\n",
    "        else:\n",
    "            vocabulary.append(word)       \n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is preprocessing step for MLE computation.\n",
    "#For unigram and bigram inbuilt NgramCounter function is used.\n",
    "#For trigram and quadgram a dictionary of all possible trigrams and quadgrams and \n",
    "#corresponding counts of each is storred.\n",
    "text_unigrams = [ngrams(vocabulary, 1)]\n",
    "text_bigrams = [ngrams(vocabulary, 2)]\n",
    "from nltk.lm import NgramCounter\n",
    "unigram_counts = NgramCounter(text_unigrams)\n",
    "bigram_counts = NgramCounter(text_bigrams)\n",
    "trigram_dict = {}\n",
    "for i in range(len(vocabulary)-2):\n",
    "    string_ = vocabulary[i]+\" \"+vocabulary[i+1]+\" \"+vocabulary[i+2] \n",
    "    if string_ not in trigram_dict:\n",
    "        trigram_dict[string_] = 1\n",
    "    else:\n",
    "        trigram_dict[string_] += 1\n",
    "quadgram_dict = {}\n",
    "for i in range(len(vocabulary)-3):\n",
    "    string_ = vocabulary[i]+\" \"+vocabulary[i+1]+\" \"+vocabulary[i+2]+\" \"+vocabulary[i+3] \n",
    "    if string_ not in quadgram_dict:\n",
    "        quadgram_dict[string_] = 1\n",
    "    else:\n",
    "        quadgram_dict[string_] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here MLE for unigram, bigram, trigram and quadgram is calculated.\n",
    "def unigramMLE(phrase):\n",
    "    return unigram_counts[phrase]/len(vocabulary)\n",
    "def bigramMLE(phrase):\n",
    "    phrase_list = phrase.split()\n",
    "    return bigram_counts[[phrase_list[0]]][phrase_list[1]]/unigram_counts[phrase_list[0]]\n",
    "def trigram(phrase):\n",
    "    phrase_list = phrase.split()\n",
    "    return trigram_dict[phrase]/bigram_counts[[phrase_list[0]]][phrase_list[1]]\n",
    "def quadgram(phrase):\n",
    "    phrase_list = phrase.split()\n",
    "    str_ = \" \"\n",
    "    return quadgram_dict[phrase]/trigram_dict[str_.join(phrase_list[:-1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.25153374233128833\n",
      "0.021505376344086023\n",
      "0.000912270882887925\n"
     ]
    }
   ],
   "source": [
    "#Demonstration for all four MLEs.\n",
    "print(quadgram(\"goldman sachs and citibank\"))\n",
    "print(bigramMLE(\"america great\"))\n",
    "print(trigram(\"i will do\"))\n",
    "print(unigramMLE(\"america\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def generate(n):\n",
    "    #n-gram model will be used to generate text. For unigram n=1.\n",
    "    mle_dict = {}\n",
    "    # Making MLE dictionary to store all n-1 grams and corresponding next words. Hard coded.\n",
    "    for i in range(len(vocabulary)):\n",
    "        try:\n",
    "            string_=\"\"\n",
    "            for j in range(i,i+n-1):\n",
    "                if vocabulary[j]==\"</s>\":\n",
    "                    continue\n",
    "                string_ = string_+vocabulary[j]+\" \"\n",
    "            string_ = string_.strip()\n",
    "            #A list of dictionaries corresponding to each n-1 gram.\n",
    "            if string_ not in mle_dict.keys():\n",
    "                mle_dict[string_] = {vocabulary[i+n-1]:1}\n",
    "            else:\n",
    "                if vocabulary[i+n-1] not in mle_dict[string_]:\n",
    "                    mle_dict[string_][vocabulary[i+n-1]] = 1\n",
    "                else:\n",
    "                    mle_dict[string_][vocabulary[i+n-1]] += 1\n",
    "        except Exception as e:\n",
    "            break\n",
    "    st_list = []\n",
    "    for key in mle_dict:\n",
    "        ct = sum(mle_dict[key].values())\n",
    "        if key[0:3] == \"<s>\":\n",
    "            st_list.append((key, ct))\n",
    "        for i in mle_dict[key]:\n",
    "            mle_dict[key][i] /= ct\n",
    "    ct = 0\n",
    "    generated_sentence = \"\"\n",
    "    gen_prob = []\n",
    "    for i in st_list:\n",
    "        ct+=i[1]\n",
    "    for i in st_list:   \n",
    "        gen_prob.append(i[1]/ct)\n",
    "    # Using multinomial to predict next word with probabilities provided.\n",
    "    a = np.random.multinomial(1, gen_prob, size=1)\n",
    "    index = list(a[0]).index(1)\n",
    "    generated_sentence += st_list[index][0] + \" \"\n",
    "    last_ngram_pick = st_list[index][0]\n",
    "    while('</s>' not in generated_sentence[-6:-1] and len(generated_sentence)<150):\n",
    "        gen_prob = []\n",
    "        ct = sum(mle_dict[last_ngram_pick].values())\n",
    "        for key in mle_dict[last_ngram_pick]:\n",
    "            gen_prob.append(mle_dict[last_ngram_pick][key]/ct)\n",
    "        a = np.random.multinomial(1, gen_prob, size=1)\n",
    "        index = list(a[0]).index(1)\n",
    "        generated_sentence += list(mle_dict[last_ngram_pick].keys())[index] + \" \"\n",
    "    print(\"a generated sentence\",generated_sentence)\n",
    "#     print(mle_dict\n",
    "    return mle_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a generated sentence <s> we have a some tremendous no no to to no people people cities an $ to to a interest borders a an very great – 2,000 – people a great a these third \n"
     ]
    }
   ],
   "source": [
    "mle_dict = generate(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a generated sentence <s> i am totally in favor of vaccines but i want smaller doses over a longer period of time <s> so we have isis taking a lot of oil <s> now let ’ s take some questions <s> it ’ s just gross incompetence at \n",
      "final_perplexity:  3298.9986398257684\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# This function returns perplexity for ngram model. input n; prints perplexity; \n",
    "def perplexity_(n):\n",
    "    mle_dict = generate(n)\n",
    "    # First we pre process test set.\n",
    "    ls = []\n",
    "    sent_tokenize_list = [token.lower() for token in test_]\n",
    "\n",
    "    final_perplexity = 0\n",
    "    for sentence in sent_tokenize_list:\n",
    "        \n",
    "        sentence = \"!\"+sentence+\"!\"\n",
    "        #We tokenize sentence\n",
    "        words = word_tokenize(sentence)\n",
    "        words[0] = \"<s>\"\n",
    "        words[-1]=\"</s>\"\n",
    "#  \n",
    "        for i in range(len(words)):\n",
    "                perplexity = 1\n",
    "                #Intializing proplexity by 1.\n",
    "#             try:\n",
    "                last_ngram_pick = \" \".join(words[i:n-1+i])\n",
    "                if last_ngram_pick not in mle_dict:\n",
    "                    #If provided ngram doesn't exist in the train set.\n",
    "                    perplexity*= 1/len(mle_dict)\n",
    "                else:\n",
    "                    ct = sum(mle_dict[last_ngram_pick].values())\n",
    "                    for key in mle_dict[last_ngram_pick]:\n",
    "                        if key == words[i+n-1]:\n",
    "                            # Using log to operate in extreme values.\n",
    "                            perplexity += math.log2(mle_dict[last_ngram_pick][key]/ct)\n",
    "                \n",
    "        final_perplexity += (2**(perplexity*-1/i))\n",
    "\n",
    "    final_perplexity = final_perplexity\n",
    "    print(\"final_perplexity: \",final_perplexity)\n",
    "perplexity_(50)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence from trigram: \n",
    "    <s> but they the you when don it then likewise we don those so a what when there \n",
    "    they they with the you – you you i bill i you we i we he carrier likewise \n",
    "sentence from bigram:\n",
    "    <s> the you i the except so we you the i we but sometimes all and well you you i well \n",
    "    and how a so …it so but i i club people i and you i but our but </s>\n",
    "sentence from quadgram:\n",
    "    <s> and you adam the the don by i we that steve i somebody i you it then maybe you \n",
    "    what there we then i people i she then what we then you you our here \n",
    "\n",
    "Readability of trigram and quadgram is much better but still makes no sense. \n",
    "Speech corpus is comparatively small corpus, hence conventional approach is very less sensible.\n",
    "    \n",
    "Perplexity of trigram model: 3298\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
